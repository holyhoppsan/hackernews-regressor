{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup of WikiText2 Dataset\n",
    "\n",
    "Load the datasets, including the WikiText-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"}\n",
      "Flowers captivate with their myriad forms and hues, serving both as nature's art and essential elements in ecosystems. They symbolize emotions and mark significant life events across cultures. Beyond beauty, flowers play crucial roles in agriculture by attracting pollinators, essential for crop production. The floriculture industry, while economically significant, faces challenges of sustainability, prompting a shift towards environmentally friendly practices. As symbols of love, grief, and celebration, flowers bridge the gap between human expression and the natural world, reminding us of life's fleeting beauty and the importance of preserving the delicate balance of our environment.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "print(dataset['train'][4])\n",
    "\n",
    "dev_dataset_string = \"datasets/test_text.txt\"\n",
    "\n",
    "dev_dataset = \"\"\n",
    "with open(dev_dataset_string, 'r') as file:\n",
    "    dev_dataset = file.read()\n",
    "\n",
    "# Now file_content holds the entire content of the file as a string\n",
    "print(dev_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Train the sentencepiece model on the data from the test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=datasets/test_text.txt --model_prefix=m --vocab_size=100\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: datasets/test_text.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: datasets/test_text.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=693\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=32\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=303\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 209 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 80\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 80 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=137 obj=16.7969 num_tokens=283 num_tokens/piece=2.06569\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=123 obj=15.6464 num_tokens=283 num_tokens/piece=2.30081\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109 obj=17.1646 num_tokens=297 num_tokens/piece=2.72477\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=108 obj=16.9227 num_tokens=301 num_tokens/piece=2.78704\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(f\"--input={dev_dataset_string} --model_prefix=m --vocab_size=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Load the model from the model file, and encode the test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'T', 'hi', 's', '▁', 'i', 's', '▁a', '▁', 'te', 's', 't', '▁', 's', 'ent', 'e', 'n', 'ce', '.']\n",
      "This is a test sentence.\n",
      "[3, 90, 63, 24, 25, 31, 14, 83, 41, 14, 35, 28, 8, 58, 20, 8, 82, 70, 6, 23, 14, 15, 59, 75, 5, 9, 50, 34, 7, 3, 4, 61, 62, 12, 22, 18, 58, 5, 4, 55, 13, 45, 4, 3, 42, 11, 5, 9, 3, 74, 33, 8, 17, 3, 71, 69, 33, 4, 38, 43, 4, 6, 4, 35, 75, 19, 47, 6, 44, 8, 92, 13, 3, 40, 18, 29, 4, 5, 9, 70, 42, 91, 51, 54, 76, 41, 33, 4, 5, 64, 4, 4, 3, 27, 4, 19, 3, 93, 13, 6, 18, 9, 48, 7, 30, 24, 25, 32, 21, 14, 6, 31, 82, 98, 99, 8, 17, 67, 68, 34, 38, 5, 97, 23, 27, 22, 6, 5, 11, 11, 46, 12, 32, 68, 73, 87, 14, 11, 36, 4, 7, 3, 74, 33, 8, 17, 59, 3, 64, 85, 57, 15, 98, 99, 29, 19, 47, 30, 23, 27, 3, 79, 52, 82, 6, 7, 28, 60, 37, 43, 87, 18, 69, 53, 21, 21, 6, 51, 7, 72, 14, 66, 31, 88, 17, 37, 12, 34, 16, 10, 3, 4, 52, 14, 8, 87, 14, 86, 8, 73, 11, 6, 7, 57, 78, 77, 12, 5, 3, 4, 60, 10, 11, 3, 11, 18, 96, 42, 15, 4, 49, 81, 6, 72, 65, 9, 21, 6, 32, 46, 66, 19, 3, 89, 4, 44, 4, 16, 10, 3, 63, 41, 13, 7, 3, 97, 65, 10, 7, 5, 9, 31, 71, 86, 82, 14, 29, 7, 30, 24, 25, 22, 23, 15, 97, 13, 20, 3, 97, 84, 22, 13, 11, 24, 13, 87, 50, 69, 26, 76, 94, 56, 4, 8, 18, 87, 5, 9, 20, 55, 17, 28, 36, 21, 15, 7, 67, 40, 79, 8, 12, 3, 98, 4, 16, 10, 54, 45, 4, 72, 37, 13, 77, 12, 48, 5, 9, 20, 3, 8, 78, 36, 11, 26, 39, 16, 10, 3, 56, 61, 62, 12, 20, 3, 15, 13, 21, 53, 35, 22, 17, 26, 39, 16, 10, 16, 98, 82, 49, 19]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load your model\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "# Encode a sentence\n",
    "sentence = \"This is a test sentence.\"\n",
    "encoded_pieces = sp.encode(sentence, out_type=str)\n",
    "print(encoded_pieces)\n",
    "\n",
    "# Decode back to a sentence\n",
    "decoded_sentence = sp.decode(encoded_pieces)\n",
    "print(decoded_sentence)\n",
    "\n",
    "# encode the dev dataset\n",
    "encoded_dev_dataset = sp.Encode(dev_dataset)\n",
    "print(encoded_dev_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackernews-regressor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
